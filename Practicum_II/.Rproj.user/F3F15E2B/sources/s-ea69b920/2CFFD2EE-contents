---
title: "Practicum II"
output:
  html_notebook: default
  html_document:
    df_print: paged
  pdf_document: default
---

### Authors:
- Simin Zhang
- Yiman Liu
- Zhi Wen

### Tasks:

1. (0 pts / 60 min) Inspect and then download IMDB movie data from https://www.imdb.com/interfaces/ (Links to an external site.). Read the descriptions of the data subsets and then download the compressed (gz) tab-separated files (tsv). If you cannot access the link, use this Canvas resource.  

2. (10 pts) Read the data description and inspect the files, then:  

* (5 pts / 60 min) create a data model in the form of an ERD in Crow's Foot notation using a tool of your choice (e.g., LucidChart, TOAD, MySQL Workbench, etc.) and embed an image of the model in your notebook.    
  
* [ERD in LucidChart](https://lucid.app/invitations/accept/5f7a9eba-0a39-49bf-b32e-41dba8cf6f64)
![](https://raw.githubusercontent.com/yimanliu0/database/master/image/PracticumIIERD.jpeg) 

* (4 pts / 15 min) add junction/association tables to normalize many-to-many relationships, normalize multi-valued attributes, and create lookup tables for categorical attribute values. Embed an updated image of the model in your notebook.  
  
* [ERD in LucidChart in BCNF](https://lucid.app/invitations/accept/537ce00b-b074-48cc-be17-0dbd680d582f)  
![](https://raw.githubusercontent.com/yimanliu0/database/master/image/ERD%20IN%20BCNF.jpeg) 
  
* (1 pts / 5 min) add two new attributes (columns) to the appropriate tables: one for the age of a person and one for the number of movies a person has appeared in. Embed an updated image of the model in your notebook. Leave those columns empty for now. They will be filled in later.  
  
![](https://raw.githubusercontent.com/yimanliu0/database/master/image/ERD%20IN%20BCNFUpdate.jpeg)   

* (2 pts / 10 min) ensure that the relational model is in at least BCNF.   
  
  * **There is no many-to-many relationships and multi-valued attributes in this relational model.**  
  **There is no multi-valued attributes in each table, so it is in 1NF.**  
  **There is no non-key value depending on part of key, so it is in 2NF.**  
  **There is no non-key attribute can decide primary key attribute, so it is in BCNF.**

3. (8 pts / 60 min) Create and then run CREATE TABLE statements to build the schema. These statements must run from within your notebook and not from a separate script. Ensure proper referential integrity.  

```{R}
# load libraries
library(DBI)
library(odbc)

# connect to mysql
con <- DBI::dbConnect(odbc::odbc(),
                      Driver   = "MySQL",
                      Server   = "cs5200.cerugoc9p5ux.us-east-1.rds.amazonaws.com",
                      Database = "practicum-ii",
                      UID      = "admin",
                      PWD      = "cs5200db",
                      Port     = 3306)
```

```{sql, connection=con}
CREATE TABLE IF NOT EXISTS title (
  tconst VARCHAR(15) NOT NULL,
  titleType VARCHAR(15),
  primaryTitle VARCHAR(30),
  originalTitle VARCHAR(30),
  isAdult TINYINT,
  startYear INT,
  endYear INT,
  runtimeMinutes INT,
  PRIMARY KEY (tconst)
);
```

```{sql, connection=con}
CREATE TABLE IF NOT EXISTS alternativeTitle (
  tconst VARCHAR(15),
  ordering INT,
  title VARCHAR(30),
  region VARCHAR(30),
  language VARCHAR(30),
  isOriginalTitle TINYINT,
  PRIMARY KEY (tconst, ordering),
  CONSTRAINT FK_title_alternativeTitle
    FOREIGN KEY (tconst)
    REFERENCES title (tconst)
    ON DELETE RESTRICT
    ON UPDATE RESTRICT
);
```

```{sql, connection=con}
CREATE TABLE IF NOT EXISTS person (
  nconst VARCHAR(15) NOT NULL,
  primaryName VARCHAR(45),
  birthYear INT,
  deathYear INT,
  age INT,
  numOfMoviesAppeared INT,
  PRIMARY KEY (nconst)
);
```

```{sql, connection=con}
CREATE TABLE IF NOT EXISTS principalCrew (
  tconst VARCHAR(15) NOT NULL,
  ordering INT NOT NULL,
  nconst VARCHAR(15) NULL,
  category VARCHAR(45) NULL,
  job VARCHAR(45) NULL,
  characters VARCHAR(45) NULL,
  PRIMARY KEY (tconst, ordering),
  CONSTRAINT FK_title_principalCrew
    FOREIGN KEY (tconst)
    REFERENCES title (tconst)
    ON DELETE RESTRICT
    ON UPDATE RESTRICT
);
```

```{sql, connection=con}
CREATE TABLE IF NOT EXISTS episode (
  tconst VARCHAR(15) NOT NULL,
  parentTconst VARCHAR(15) NULL,
  seasonNumber INT NULL,
  episodeNumber INT NULL,
  PRIMARY KEY (tconst),
  CONSTRAINT FK_title_episode
    FOREIGN KEY (tconst)
    REFERENCES title (tconst)
    ON DELETE RESTRICT
    ON UPDATE RESTRICT,
  CONSTRAINT FK_title_episode2
    FOREIGN KEY (parentTconst)
    REFERENCES title (tconst)
    ON DELETE RESTRICT
    ON UPDATE RESTRICT
);
```

```{sql, connection=con}
CREATE TABLE IF NOT EXISTS rating (
  tconst VARCHAR(15) NOT NULL,
  averageRating FLOAT NULL,
  numVotes INT NULL,
  PRIMARY KEY (tconst),
  CONSTRAINT FK_title_rating
    FOREIGN KEY (tconst)
    REFERENCES title (tconst)
    ON DELETE RESTRICT
    ON UPDATE RESTRICT
);
```

```{sql, connection=con}
CREATE TABLE IF NOT EXISTS genre (
  tconst VARCHAR(15) NOT NULL,
  genreType VARCHAR(45) NOT NULL,
  PRIMARY KEY (tconst, genreType),
  CONSTRAINT FK_title_genre
    FOREIGN KEY (tconst)
    REFERENCES title (tconst)
    ON DELETE RESTRICT
    ON UPDATE RESTRICT
);
```

```{sql, connection=con}
CREATE TABLE IF NOT EXISTS titleType (
  tconst VARCHAR(15) NOT NULL,
  ordering INT NOT NULL,
  movieType VARCHAR(20),
  PRIMARY KEY (tconst, ordering, movieType),
  CONSTRAINT FK_alternativeTitle_titleType
    FOREIGN KEY (tconst)
    REFERENCES alternativeTitle (tconst)
    ON DELETE RESTRICT
    ON UPDATE RESTRICT
);
```

```{sql, connection=con}
CREATE TABLE IF NOT EXISTS attribute (
  tconst VARCHAR(15) NOT NULL,
  ordering INT NOT NULL,
  attributeType VARCHAR(20),
  PRIMARY KEY (tconst, ordering, attributeType),
  CONSTRAINT FK_alternativeTitle_attribute
    FOREIGN KEY (tconst)
    REFERENCES alternativeTitle (tconst)
    ON DELETE RESTRICT
    ON UPDATE RESTRICT
);
```

```{sql, connection=con}
CREATE TABLE IF NOT EXISTS directorWithTitle (
  tconst VARCHAR(15) NOT NULL,
  nconst VARCHAR(15) NOT NULL,
  PRIMARY KEY (tconst, nconst),
  CONSTRAINT FK_title_directorWithTitle
    FOREIGN KEY (tconst)
    REFERENCES title (tconst)
    ON DELETE RESTRICT
    ON UPDATE RESTRICT,
  CONSTRAINT FK_person_directorWithTitle
    FOREIGN KEY (nconst)
    REFERENCES person (nconst)
    ON DELETE RESTRICT
    ON UPDATE RESTRICT
);
```

```{sql, connection=con}
CREATE TABLE IF NOT EXISTS writerWithTitle (
  tconst VARCHAR(15) NOT NULL,
  nconst VARCHAR(15) NOT NULL,
  PRIMARY KEY (tconst, nconst),
  CONSTRAINT FK_title_writerWithTitle
    FOREIGN KEY (tconst)
    REFERENCES title (tconst)
    ON DELETE RESTRICT
    ON UPDATE RESTRICT,
  CONSTRAINT FK_person_writerWithTitle
    FOREIGN KEY (nconst)
    REFERENCES person (nconst)
    ON DELETE RESTRICT
    ON UPDATE RESTRICT
);
```

```{sql, connection=con}
CREATE TABLE IF NOT EXISTS personKnownForMovies (
  nconst VARCHAR(15) NOT NULL,
  tconst VARCHAR(15) NOT NULL,
  PRIMARY KEY (nconst, tconst),
  CONSTRAINT FK_title_personKnownForMovies
    FOREIGN KEY (tconst)
    REFERENCES title (tconst)
    ON DELETE RESTRICT
    ON UPDATE RESTRICT,
  CONSTRAINT FK_person_personKnownForMovies
    FOREIGN KEY (nconst)
    REFERENCES person (nconst)
    ON DELETE RESTRICT
    ON UPDATE RESTRICT
);
```

```{sql, connection=con}
CREATE TABLE IF NOT EXISTS primaryProfession (
  nconst VARCHAR(15) NOT NULL,
  professionType VARCHAR(45) NOT NULL,
  PRIMARY KEY (nconst, professionType),
  CONSTRAINT FK_person_primaryProfession
    FOREIGN KEY (nconst)
    REFERENCES person (nconst)
    ON DELETE RESTRICT
    ON UPDATE RESTRICT
);
```


```{sql, connection=con}
SHOW TABLES;
```

4. (10 pts / 120 min) Load the table from the downloaded data files into the tables. Properly parse the foreign keys and attributes and ensure that the data is in the right tables in the right form and that referential integrity is ensured. This will require parsing code in your chosen programming language. You should create a subset of each dataset for testing (as the data sets are very large and take significant time to load). What is the effect of the referential integrity checking while you load? Can you do something about that? Describe and implement any improvement strategies.  

##### Referential integrity
1. For the data from the same dataset, the referential integrity is guaranteed.   
_e.g._ The following two records in dataset `title.akas.tsv.gz`:

| tconst    | titleType | primaryTitle           | originalTitle          | isAdult | startYear | endYear | runtimeMinutes | genres            |
|-----------|-----------|------------------------|------------------------|---------|-----------|---------|----------------|-------------------|
| tt0000001 | short     | Carmencita             | Carmencita             | 0       | 1894      | NA      | 1              | Documentary,Short |
| tt0000002 | short     | Le clown et ses chiens | Le clown et ses chiens | 0       | 1892      | NA      | 5              | Animation,Short   |


We separate the data as following and insert them into different tables `title` and `genre`:   

In `title`:

| tconst    | titleType | primaryTitle           | originalTitle          | isAdult | startYear | endYear | runtimeMinutes |
|-----------|-----------|------------------------|------------------------|---------|-----------|---------|----------------|
| tt0000001 | short     | Carmencita             | Carmencita             | 0       | 1894      | NA      | 1              |
| tt0000002 | short     | Le clown et ses chiens | Le clown et ses chiens | 0       | 1892      | NA      | 5              |

In `genre`:

| tconst    | genres      |
|-----------|-------------|
| tt0000001 | Documentary |
| tt0000001 | Short       |
| tt0000002 | Animation   |
| tt0000002 | Short       |

Since they are from the same dataset, the field `tconst` in table `genre` can always be found in table `title`.

2. For the data from the different dataset, the foreign key constraint is enforced.
_e.g._ Ratings (title.basics.tsv.gz) don't make sense if their refereced titles don't exist, therefore a foreign key constraint is created on `rating.tconst` referening to `title.tconst` to ensure the referential integrity.

##### Improvement
Since the dataset is too large, instead of loading a whole file at once, we import it in chunks (10,000 records at a time), which put less pressure on transmission.

```{R loading data}
# load libraries
library(data.table)
library(R.utils)
library(tidyr)
library(dplyr)

# get a new connection
getConnection <- function() {
  con <- dbConnect(MySQL(),
                    user = "admin",
                    password = "cs5200db",
                    dbname = "practicum-ii",
                    host = "cs5200.cerugoc9p5ux.us-east-1.rds.amazonaws.com",
                    port = 3306
                  )
	return(con)
}

# batch load function
batchInsert <- function(tablename, data){
  batchsize = 10000
  if (nrow(data) >= batchsize) {
      batch <- split(data, 1:nrow(data) %/% batchsize)
      op <- lapply(batch, function(x) {
        ## Make separate connections for each batch
        con <- getConnection()
        dbAppendTable(con, tablename, x)
        dbDisconnect(con)
      })
  } else {
    con <- getConnection()
    dbAppendTable(con, tablename, data)
    dbDisconnect(con)
  }
}

# load data

# title.basics === load
titleBasics <- fread(file = 'data/title.basics.tsv.gz', header = TRUE, na.strings='\\N', quote="")

# title
title <- titleBasics %>% select(1:8)
batchInsert('title', title)
rm(title)

# genre
genre <- titleBasics[!is.na(titleBasics$genres)] %>% select('tconst', 'genres')
names(genre)[names(genre) == 'genres'] <- 'genreType'
genre <- genre %>% separate_rows(genreType)
batchInsert('genre', genre)
rm(genre)

# title.basics === cleanup
rm(titleBasics)

# title.akas === load
titleAkas <- fread(file = 'data/title.akas.tsv.gz', header = TRUE, na.strings='\\N', quote="")
names(titleAkas)[names(titleAkas) == 'titleId'] <- 'tconst'

# alternativeTitle
alternativeTitle <- titleAkas %>% select(1:5,8)
batchInsert('alternativeTitle', alternativeTitle)
rm(alternativeTitle)

# titleType
titleType <- titleAkas[!is.na(titleAkas$types)] %>% select(tconst, ordering, types)
names(titleType)[names(titleType) == 'types'] <- 'movieType'
titleType <- titleType %>% separate_rows(movieType)
batchInsert('titleType', titleType)
rm(titleType)

# attribute
attribute <- titleAkas[!is.na(titleAkas$attributes)] %>% select(tconst, ordering, attributes)
names(attribute)[names(attribute) == 'attributes'] <- 'attributeType'
attribute <- attribute %>% separate_rows(attributeType)
batchInsert('attribute', attribute)
rm(attribute)

# title.akas === cleanup
rm(titleAkas)

# episode
episode <- fread(file = 'data/title.episode.tsv.gz', header = TRUE, na.strings='\\N', quote="")
batchInsert("episode", episode)
rm(episode)

# rating
rating <- fread(file = 'data/title.ratings.tsv.gz', header = TRUE, na.strings='\\N', quote="")
batchInsert("rating", rating)
rm(rating)

# name.basics === setup
nameBasics <- fread(file = 'data/name.basics.tsv.gz', header = TRUE, na.strings='\\N', quote="")

# person
person <- nameBasics %>% select(nconst, primaryName, birthYear, deathYear)
batchInsert("person", person)
rm(person)

# primaryProfession 
primaryProfession <- nameBasics[nameBasics$primaryProfession != ''] %>% select(nconst, primaryProfession)
names(primaryProfession)[names(primaryProfession) == 'primaryProfession'] <- 'professionType'
primaryProfession <- primaryProfession %>% separate_rows(professionType)
batchInsert('primaryProfession', primaryProfession)
rm(primaryProfession)

# personKnownForMovies
personKnownForMovies <- nameBasics[!is.na(nameBasics$knownForTitles)] %>% select(nconst, knownForTitles)
names(personKnownForMovies)[names(personKnownForMovies) == 'knownForTitles'] <- 'tconst'
personKnownForMovies <- personKnownForMovies %>% separate_rows(tconst)
batchInsert('personKnownForMovies', personKnownForMovies)
rm(personKnownForMovies)

# name.basics === cleanup
rm(nameBasics)

# crew === setup
crew <- fread(file = 'data/title.crew.tsv.gz', header = TRUE, na.strings='\\N', quote="")

# directorWithTitle
director <- crew[!is.na(crew$directors)] %>% select(tconst, directors)
names(director)[names(director) == 'directors'] <- 'nconst'
director <- director %>% separate_rows(nconst)
batchInsert('directorWithTitle', director)
rm(director)

# writerWithTitle
writer <- crew[!is.na(crew$writers)] %>% select(tconst, writers)
names(writer)[names(writer) == 'writers'] <- 'nconst'
writer <- writer %>% separate_rows(nconst)
batchInsert('writerWithTitle', writer)
rm(writer)

# crew === cleanup
rm(crew)

# principalCrew
principalCrew <- fread(file = 'data/title.principals.tsv.gz', header = TRUE, na.strings='\\N', quote="")
batchInsert('principalCrew', principalCrew)
rm(principalCrew)
```

5. (5 pts / 30 min) After loading the data, execute UPDATE statements for the two newly created columns in (2C). You may interpret what appearing in movies means and what you classify as movies -- just make it clear in your notebook.

##### Definition
A person's `numOfMoviesAppeared` is the number of titles, of which type is `movie`, he or she appeared as a principal cast/crew for.

```{sql, connection=con}
-- disable safe update mode
SET SQL_SAFE_UPDATES = 0;
```

```{sql, connection=con}
-- update the age of person
UPDATE person
SET age = IF(deathYear IS NOT NULL,
   deathYear,
   YEAR(CURDATE())
   ) - birthYear
WHERE birthYear IS NOT NULL;
```

![](https://raw.githubusercontent.com/yimanliu0/database/master/image/picFromzac.jpg)

```{sql, connection=con}
-- update the number movies appeared
UPDATE person p
INNER JOIN (
	SELECT
		a.nconst,
		IF(b.num IS NULL, 0, b.num) numOfMoviesAppeared
	FROM person a
	LEFT JOIN (
		SELECT nconst, count(tconst) num
		FROM principalCrew
		WHERE tconst IN (SELECT tconst FROM title where titleType='movie')
		GROUP BY nconst
	) b
	ON a.nconst = b.nconst
) m
ON p.nconst = m.nconst
SET p.numOfMoviesAppeared = m.numOfMoviesAppeared
```

_Since the dataset is too large, we test this update statement in a subset. It works as expected._
![](https://raw.githubusercontent.com/yimanliu0/database/master/image/zac.jpg)

6. (10 pts / 90 min) Add triggers to the appropriate tables so that the newly created columns in (2C) are automatically updated when new data in inserted.  

```{sql, connection=con}
DROP TRIGGER IF EXISTS person_before_insert;
```

**It seems like when running SQL in R, no need to change DELIMITER when creating trigger, will add DELIMITER if I run this outside R**  

```{sql, connection=con}
CREATE TRIGGER person_before_insert BEFORE INSERT ON person 
FOR EACH ROW
BEGIN
    DECLARE numOfMovies INT;
    
    SELECT COUNT(tconst) INTO numOfMovies
    FROM principalCrew 
    WHERE principalCrew.nconst = new.nconst
    AND principalCrew.tconst IN
      (SELECT title.tconst 
       FROM title 
       WHERE title.titleType = 'movie')
    GROUP BY principalCrew.nconst;
    
    SET new.numOfMoviesAppeared = numOfMovies;
    
    IF (new.deathYear IS NULL) THEN
      SET new.age = YEAR(CURDATE()) - new.birthYear;
    ELSE 
      SET new.age = new.deathYear - new.birthYear;
      END IF;
END;
```
  

**I used a small subset data in my local mysql database to run this trigger and did the query to prove this trigger works correctly because the original data set is too big that I always got timeout error whenever I wanted to drop a trigger, drop a view or do query from a view even if I changed the timeout configuration in MySql workbench.**  

**Here is my test in my local database:**  

**1.Added trigger using the same statement as above**    

![](https://raw.githubusercontent.com/yimanliu0/database/master/image/runTrigger2.png)  

**2.Insert new record into person table, then the newly created columns (age and numOfMoviesAppeared) are automatically updated**  

![](https://raw.githubusercontent.com/yimanliu0/database/master/image/queryResult.jpeg)  

7. (8 pts / 60 min) Create a view that lists the name of each actor, their age, whether they are dead or not, and how many movies they are known for.  

```{sql, connection=con}
DROP TABLE IF EXISTS death;

-- First, create a table containing isDead attribute
CREATE TABLE IF NOT EXISTS death (
  nconst VARCHAR(15),
  deathYear INT,
  isDead TINYINT
);
```

```{sql, connection=con}
DROP TRIGGER IF EXISTS death_before_insert;

-- Second, create a trriger to set isDead value according to deadYear
CREATE TRIGGER death_before_insert BEFORE INSERT ON death 
FOR EACH ROW
BEGIN
    IF(NEW.deathYear IS NULL) THEN
        SET NEW.isDead = 0;
    ELSE
        SET NEW.isDead = 1;
    END IF;
END;
```

```{sql, connection=con}
DROP VIEW IF EXISTS actor;

-- Finally, create a view containing name, age, isDead, numOfMoviesKnownFor
CREATE VIEW actor AS 
SELECT DISTINCT
       person.primaryName AS name,
       person.age AS age,
       death.isDead AS isDead,
       (SELECT COUNT(tconst) 
       FROM personKnownForMovies
       WHERE personKnownForMovies.nconst = person.nconst
       AND personKnownForMovies.tconst IN (SELECT tconst FROM title WHERE titleType = 'movie')
       GROUP BY personKnownForMovies.nconst) AS numOfMoviesKnownFor
FROM person
INNER JOIN death
ON person.nconst = death.nconst
INNER JOIN principalCrew
ON person.nconst = principalCrew.nconst 
AND principalCrew.category = 'actor' OR principalCrew.category = 'actress';
```

```{R}
# construct a SQL query
sqlCmd = "SELECT * FROM actor"

# send the SQL query to the database
rs = dbGetQuery(con, sqlCmd)

# extract records
dbFetch(rs)

# finish fetching the records
dbClearResult(rs)

```

**Note: the result is based on a small data sample. I also assume 'short' is a kind of movie as there was 'short film' in last century**

**1.Create a view into the database using the same statement as above.**

![](https://raw.githubusercontent.com/yimanliu0/database/master/image/createView.png)

**2.Get a view containing the name of each actor/actress, their age, whether they are dead or not, and how many movies they are known for.**

![](https://raw.githubusercontent.com/yimanliu0/database/master/image/viewResult.png)

8. (9 pts / 90 min) Write a query that finds the number of seasons for each TV series. Using the results of the query create a histogram (frequency plot).  

```{R}
# load libraries
library(ggplot2)

# build query
query <- dbSendQuery(con, 
"SELECT
  parentTconst,
  count(DISTINCT seasonNumber) numberOfSeasons
FROM episode
GROUP BY parentTconst")

# process data
df <- dbFetch(query)
df['numberOfSeasons'] <- lapply(df['numberOfSeasons'], as.numeric)
head(df)

# plot
ggplot(df, aes(numberOfSeasons)) +
  geom_bar()
# number of seasons under 15
ggplot(df, aes(numberOfSeasons)) +
  geom_bar() +
  xlim(0, 15)
```

9. (10 pts / 120 min) Build a function in your code or a stored procedure in the database (approach is your choice) called addActor() that adds a new actor to the database: this requires updating several tables, so the insertions must occur within a transaction in order to function properly in a concurrent environment. Test your function by inserting a new actor -- you may make up a name and associated information. Show evidence in your notebook that the actor was properly inserted.  


```{sql, connection=con}

DROP PROCEDURE IF EXISTS addActor;


```

```{sql, connection=con}
-- DELIMITER $$
-- If use this stored procedure outside of R like MYSQL, we need to add Delimiter $$

CREATE PROCEDURE addActor(IN p_nconst VARCHAR(15), IN p_primaryName VARCHAR(45), IN p_birthYear INT, IN p_deathYear INT, IN p_age INT, IN p_numOfMoviesAppeared INT, IN p_professionType VARCHAR(45), IN p_tconst VARCHAR(15), IN p_ordering INT,IN p_job VARCHAR(45), IN  p_characters VARCHAR(45))
BEGIN
START TRANSACTION;

	INSERT INTO person (nconst, primaryName, birthYear, deathYear, age, numOfMoviesAppeared) 
		VALUES (p_nconst, p_primaryName, p_birthYear, p_deathYear, p_age, p_numOfMoviesAppeared);
		
        
	INSERT INTO primaryProfession(nconst, professionType) 
		VALUES (p_nconst, p_professionType);
        
        
	INSERT INTO personKnownForMovies(nconst, tconst)
		VALUES (p_nconst, p_tconst);
        
	
  INSERT INTO principalCrew (tconst, ordering, nconst, category, job, characters)
		VALUES (p_tconst, p_ordering, p_nconst, p_professionType, p_job, p_characters);
        
	
    
    
    COMMIT;
END;
-- END$$
-- If use this stored procedure outside of R like MYSQL, after we declare DELIMITER $$ in the beginning, we need to change END; to END$$

-- If use this stored procedure outside of R like MYSQL, after we declare DELIMITER $$ in the beginning, we need to change END; to END$$. And, we also need change back the DELIMITER to ; .
-- DELIMITER ;
```

**Test: addActor()**
```{sql, connection=con}

CALL addActor("nm010", "Red Buttons", 1919, 2006, 87, NULL, "writer", "tt001", 3, NULL, "self");

```

**Because it always lost connection, I used my local database to test the query.**

**TABLE - person before CALL addActor() stored procedure**
![View result](https://raw.githubusercontent.com/Simin-Zhang/practicum2/main/9.1.png)

**TABLE - primaryProfession before CALL addActor() stored procedure**
![View result](https://raw.githubusercontent.com/Simin-Zhang/practicum2/main/9.2.png)


**TABLE - personKnownForMovies before CALL addActor() stored procedure**
![View result](https://raw.githubusercontent.com/Simin-Zhang/practicum2/main/9.3.png)



**TABLE - principalCrew before CALL addActor() stored procedure**
![View result](https://raw.githubusercontent.com/Simin-Zhang/practicum2/main/9.4.png)



**CALL addActor() stored procedure**
![CALL addActor() stored procedure](https://raw.githubusercontent.com/Simin-Zhang/practicum2/main/9.5.png)



**TABLE - person after CALL addActor() stored procedure, a new added value - Red Buttons**
![View result](https://raw.githubusercontent.com/Simin-Zhang/practicum2/main/9.6.png)
**TABLE - primaryProfession after CALL addActor() stored procedure, nconst: nm010 and professionType: writer**
![View result](https://raw.githubusercontent.com/Simin-Zhang/practicum2/main/9.7.png)
**TABLE - principalCrew after CALL addActor() stored procedure, tconst: tt001, ordering: 3, nconst: nm010, category: writer, job:NULL, characters: self **
![View result](https://raw.githubusercontent.com/Simin-Zhang/practicum2/main/9.8.png)
**TABLE - personKnownForMovies after CALL addActor() stored procedure, nconst: nm010, tconst: tt001**
![View result](https://raw.githubusercontent.com/Simin-Zhang/practicum2/main/9.9.png)

**According to the result showing above, the stored procedure - addActor() works successfully. The stored procedure in the database updates seveal tables and occur within a transaction in order.**


10. (5 pts / 60 min) Build a function in your code or a stored procedure in the database (approach is your choice) called deleteActor() that removes an actor from the database: this requires updating several tables, so the deletions must occur within a transaction in order to function properly in a concurrent environment. Test your function by deleting a new actor inserted in (9) -- show evidence that the removal was successful.  

```{sql, connection=con}

DROP PROCEDURE IF EXISTS deleteActor;


```

**Delete actor based on the nconst**
```{sql, connection=con}
-- DELIMITER $$
-- If use this stored procedure outside of R like MYSQL, we need to add Delimiter $$

CREATE PROCEDURE  deleteActor(IN p_nconst VARCHAR(15))
BEGIN

START TRANSACTION;
	DELETE FROM personKnownForMovies
	WHERE 
	personKnownForMovies.nconst = p_nconst;
  
	
	DELETE FROM primaryProfession
	WHERE
	primaryProfession.nconst = p_nconst;
    	
  
  DELETE FROM principalCrew
  WHERE 
	principalCrew.nconst = p_nconst;
    
    
	DELETE FROM person
	WHERE 
	person.nconst = p_nconst;
    
    


COMMIT;

END;

-- END$$
-- If use this stored procedure outside of R like MYSQL, after we declare DELIMITER $$ in the beginning, we need to change END; to END$$

-- If use this stored procedure outside of R like MYSQL, after we declare DELIMITER $$ in the beginning, we need to change END; to END$$. And, we also need change back the DELIMITER to ; .
-- DELIMITER ;

```

```{sql, connection=con}

CALL deleteActor("nm010");

```

**TABLE - person after CALL deleteActor() stored procedure, the value added in task 9 is be removed after using CALL deleteActor("nm010"); More detail please look at the history, which is located in the bottom of image.**
![View result](https://raw.githubusercontent.com/Simin-Zhang/practicum2/main/10.1.png)
**TABLE - personKnownForMovies after using deleteActor() stored procedure stored procedure. More detail please look at the history, which is located in the bottom of image.**
![View result](https://raw.githubusercontent.com/Simin-Zhang/practicum2/main/10.2.png)
**TABLE - primaryProfession after using deleteActor() stored procedure stored procedure. More detail please look at the history, which is located in the bottom of image.**
![View result](https://raw.githubusercontent.com/Simin-Zhang/practicum2/main/10.3.png)
**TABLE - principalCrew after using deleteActor() stored procedure stored procedure. More detail please look at the history, which is located in the bottom of image.**
![View result](https://raw.githubusercontent.com/Simin-Zhang/practicum2/main/10.4.png)

11. (10 pts / 45 min) Write a query to retrieve the names and ages of all actors who appeared in more than two movies (but not TV series) which an above average rating. Show the results of the query in your notebook.  


```{sql, connection=con}
SELECT (AVG(averageRating))
FROM rating;
```

**This is the query to get the average rating of all movies, and the result is 6.88682119047735. I will apply this result to the next query **
![View result](https://raw.githubusercontent.com/Simin-Zhang/Database/main/11.1.1.png)



**the value of rating.averageRating is from the result of the first query: SELECT (AVG(averageRating)) FROM rating;**

```{sql, connection=con}
SELECT person.primaryName, person.age 
From person 
WHERE
person.nconst
IN
(SELECT person.nconst
FROM person
JOIN primaryProfession ON primaryProfession.nconst = person.nconst
JOIN personKnownForMovies ON personKnownForMovies.nconst = person.nconst
JOIN title ON title.tconst = personKnownForMovies.tconst
JOIN rating ON rating.tconst = title.tconst
WHERE
title.titleType = 'movie'
AND
primaryProfession.professionType = 'actor'
AND
rating.averageRating > 6.88682119047735
GROUP BY person.nconst
HAVING (COUNT(*)  > 2));
```

 ![View result](https://raw.githubusercontent.com/Simin-Zhang/Database/main/11.2.png)

12. (5 pts / 90 min) Write a query that finds an actor by name (pick a name). Measure the execution time of the query. Then create an index that would improve the performance of the query and then run and measure it again. Show the difference in a bar chart and comment on why that's the case.  

**Before creating index**

```{sql, connection=con}
SELECT * FROM person
JOIN primaryProfession ON primaryProfession.nconst = person.nconst
WHERE
person.primaryName = "James Dean"
AND
primaryProfession.professionType = "actor";
```


**The first action of the Action Output is the duration of the query before creating Index. And, the duration is 64.174 sec. The second action shows that it successfully creates a index: person_primaryName_index**
![CREATE INDEX](https://raw.githubusercontent.com/Simin-Zhang/Database/main/12.2.png)


```{sql, connection=con}
DROP INDEX person_primaryName_index ON person;
```

**Creating index**

```{sql, connection=con}
CREATE INDEX person_primaryName_index ON person(primaryName);
```


**After creating index**
```{sql, connection=con}
EXPLAIN ANALYZE SELECT * FROM person
JOIN primaryProfession ON primaryProfession.nconst = person.nconst
WHERE
person.primaryName = "James Dean"
AND
primaryProfession.professionType = "actor";
```

**This third action is the duration of the query after creating Index, the duration is 0.371 sec. The detail shows on the third of Action Output. **
![View result](https://raw.githubusercontent.com/Simin-Zhang/Database/main/12.4.png)

**show difference in bar chart**


```{R}
secs <- c(64.174,0.371 )
barplot(secs,
main = "The execution time of the query",
xlab = "Query",
ylab = "Duration in seconds",
ylim = c(0,70),
names.arg = c("Before adding Index", "After adding Index"),
col = "lightblue")
ylim = c(0,70)
text(secs, labels=secs, cex=1, pos=3, col="black")

```

**Comment on why that's the case?**
**It is because index can help to speed up data search and SQL query performance.The database indexes reduce the number of data pages that have to be read in order to find the specific record. According to the results, it saves significant time after using index. **

13. (5 pts / 90 min) Add several indexes to a table of your choice (one containing lots of rows) and then insert additional rows into the table. Provide measurements of insert performance with no additional index, one, two, three, etc. Plot the performance change in a line graph and comment on the difference.  

```{R}
# insert with 0 index
start_time <- Sys.time()
dbSendQuery(con,"
  INSERT INTO title (tconst, titleType, primaryTitle,
  originalTitle, isAdult, startYear, runtimeMinutes)
  VALUES (\'tt9916990\', \'short\', \'Tomorrow\', \'See you again\', 0, 1990, 5);
")
end_time <- Sys.time()
zeroIndexTime = end_time - start_time
print(zeroIndexTime)

# add index
dbSendQuery(con, "
  CREATE INDEX index_primaryTitle
  ON title (primaryTitle);
")

# insert with 1 index
start_time <- Sys.time()
dbSendQuery(con,"
  INSERT INTO title (tconst, titleType, primaryTitle,
  originalTitle, isAdult, startYear, runtimeMinutes)
  VALUES (\'tt9916991\', \'short\', \'Long River\', \'Paris\', 0, 1995, 3);
")
end_time <- Sys.time()
oneIndexTime = end_time - start_time

# add index
dbSendQuery(con, "
  CREATE INDEX index_originalTitle
  ON title (originalTitle);
")

# insert with 2 indexes
start_time <- Sys.time()
dbSendQuery(con,"
  INSERT INTO title (tconst, titleType, primaryTitle,
  originalTitle, isAdult, startYear, runtimeMinutes)
  VALUES (\'tt9916992\', \'short\', \'Sunshine\', \'Sunshine\', 0, 2001, 5);
")
end_time <- Sys.time()
twoIndexTime = end_time - start_time

# add index
dbSendQuery(con, "
  CREATE INDEX index_startYear
  ON title (startYear);
")

# insert with 3 indexes
start_time <- Sys.time()
dbSendQuery(con,"
  INSERT INTO title (tconst, titleType, primaryTitle,
  originalTitle, isAdult, startYear, runtimeMinutes)
  VALUES (\'tt9916993\', \'short\', \'Youngh\', \'Be your side\', 0, 2002, 1);
")
end_time <- Sys.time()
threeIndexTime = end_time - start_time

# Create the data for the graph.
v <- c(zeroIndexTime, oneIndexTime, twoIndexTime, threeIndexTime)

# Plot the line graph.
plot(v,type = "o", col = "red", xlab = "insertion times", ylab = "runtime",
   main = "Runtime of insertion with different number of indexes")
names=c('zeroIndex', 'oneIndex', 'twoIndex', 'ThreeIndex')
text(v, labels=names, cex=0.7, pos=3, col="blue")
```

**As the original data set is too large, I always got timeout error when executing the above code, so I run the above queries in my local database (a small sample data set)**  
**1.Insertion with zero Index**  
![](https://raw.githubusercontent.com/yimanliu0/database/master/image/zeroIndex.jpeg)
**2.Insertion with one index** 
![](https://raw.githubusercontent.com/yimanliu0/database/master/image/oneIndex.jpeg)
**3.Insertion with two indexes**  
![](https://raw.githubusercontent.com/yimanliu0/database/master/image/twoIndex.jpeg)

**4.Insertion with three indexes**
![](https://raw.githubusercontent.com/yimanliu0/database/master/image/threeIndex.jpeg)

```{R}
# Plot the line graph using the above data.
v <- c(2.933, 2.732, 9.718, 13.411)
plot(v,type = "o", col = "red", xlab = "insertion times", ylab = "runtime (millisecond)", xlim = c(0, 5.0), ylim = c(0, 15.0),
   main = "Runtime of insertion with different number of indexes")
names=c('zeroIndex', 'oneIndex', 'twoIndex', 'ThreeIndex')
text(v, labels=names, cex=0.7, pos=3, col="blue")
```
**There is a deviation between zeroIndex and oneIndex. I think the runtime for insertion with one index should take longer than insertion with zero index, so I believe the deviation is because the data set is small, not big enough to demonstrate the result. However, the overall result trend is increasing with the increase of number of indexes in the table.**

**Why did the performance change with different number of indexes?**  

**The more indexes a table has, the slower the insertion executes because adding a new row to a table is more complex than regular insertion. The database must find a place to store the new inserted record, but without index, the new row can be inserted into any places. However, with indexes, the database must make sure the new row can also be found via these indexes, so it takes time to place the new record.Adding a single index will increase the execute time by a factor of a hundred. Thus, each additional index slows the execution down further.**  

14. (5 pts / ongoing) Proper documentation and clean code that is easy to follow. Professional preparation of the work.